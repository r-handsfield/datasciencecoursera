---
title: "c6w2l2"
author: "Robert Handsfield"
date: "04/19/2015"
output:
  html_document:
    highlight: haddock
    number_sections: yes
    toc: yes
---

```{r multiplotFunction, echo=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

# 05 The Variance

## 0501 The Variance

Variance measures a random variable's spread about it's mean. 

$$Var(X) = E[(X-\mu)^2] = E[X^2] - E[X]^2$$

Variance is converted to standard deviation simply to move from units of $X^2$ to units of $X$.

$$SD(X) = \sqrt{Var(X)}$$

## Examples of Calculating the Variance

### Example 1 - Calculate the Variance of a Die Roll

$$Var(X) = E[X^2] - E[X]^2$$

$E[X]=3.5$

$E[X^2] = 1^2 \times {1 \over 6} + 2^2 \times {1 \over 6} + 3^2 \times {1 \over 6} + 4^2 \times {1 \over 6} + 5^2 \times {1 \over 6} + 6^2 \times {1 \over 6} = 15.17$

$Var(X) = E[X^2] - E[X]^2 = 15.17 - 3.5^2 \approx  2.92$


### Example 2 - Calculate the Variance of a Coin Toss
What's the variance from the result of a coin toss with probability of heads(1) of $p$?

$$Var(X) = E[X^2] - E[X]^2$$

$E[X] = 0 \times (1 - p) + 1 \times p = p$

$E[X^2] = 0^2 \times (1-p) + 1^2 \times p = p$ 

$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$


## 0502 Sample Variance vs Population Variance
Just like a sample's mean is analogous to its population mean:

1. The population mean is the center (of mass) of the _population distribution_
2. The sample mean is the center (of mass) of the _observed data_
3. The sample mean is always an estimate of the population mean
4. The sample mean is unbiased
	+ The population mean of the sample mean distribution is the mean that the sample mean is trying to estimate
5. The more observations in the sample mean, the more its mass/density function concentrates around the population mean

A sample's variance is analogous to its population variance:

1. The population variance is the expected squared distance between a random variable and the population mean
	+ $Var(X) = \sigma^2 = {\frac{\sum_{i=1} (X_i - \mu)^2}{n-1}} = E[(X-\mu)^2] = E[X^2] - E[X]^2$
2. The sample variance is the expected squared distance between the observed values and the sample mean
	+ $S^2 = Var({X_s}) = \frac{\sum_{i=1} (X_{si} - \bar X_s)^2}{n-1}$
3. The sample variance is also a random variable with a population distribution
4. The expected value of the sample variance converges to the population variance as sample size increases
	+ See _Example 4_
5. The sample variance estimates the population variance
	+ As sample size increases, the distribution of the samples' variance values collapses around the population variance

> __Sample Variance__ means _The Variance of One Sample from a Population_
> $$S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}$$


### Example 3 - Simulating from a population with variance 1

1. Simulate 10 standard normal distributions
2. Calculate & plot the sample variance: $Var(X) = E[X^2] - E[X]^2$
3. Repeat 10,000 times
4. Repeat for 20 and 30 standard normals

The mean of each set of samples' variances is at 1: $E[Var(X)] = 1$, which we expect. As we sample from more standard normals, the variance of the samples' variance collapses around 1; the estimate becomes more precise.

> More data yields a better, more concentrated estimate around whatever the samples' variance is trying to estimate.

Plotting the variance distribution from 10 standard normals gives the lowest curve, 20 gives the middle, 30 gives the top curve.

```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
library(ggplot2)
nosim <- 10000; 
dat <- data.frame(
    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),
          apply(matrix(rnorm(nosim * 20), nosim), 1, var),
          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),
    n = factor(rep(c("10", "20", "30"), c(nosim, nosim, nosim))) 
    )
ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 2, alpha = .2) + geom_vline(xintercept = 1, size = 2) + xlab("Variance Value")

```


### Example 4 - Variances of x die rolls
The variance of a die roll is $2.92$ (from example 1).

$E[X]=3.5$

$E[X^2] = 1^2 \times {1 \over 6} + 2^2 \times {1 \over 6} + 3^2 \times {1 \over 6} + 4^2 \times {1 \over 6} + 5^2 \times {1 \over 6} + 6^2 \times {1 \over 6} = 15.17$

$Var(X) = E[X^2] - E[X]^2 = 15.17 - 3.5^2 \approx  2.92$

1. Roll a sample of 10 dice
2. Find the variance of the 10 results
3. Repeat 10,000 times 
4. Plot the 10,000 variance values
	+ (each of these values is a sample variance $S^2$)
5. The mean value of the resulting distribution will be $2.92$


```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(ggplot2)
nosim <- 10000;
dat <- data.frame(
  x = c(
  	apply(
  	      	matrix(
  			sample(1 : 6, nosim * 10, replace = TRUE), nosim), 
  	      	1, var),
  	
        apply(
              	matrix(
              	     	sample(1 : 6, nosim * 20, replace = TRUE), nosim), 
              	1, var),
        
        apply(
        	matrix(
       			sample(1 : 6, nosim * 30, replace = TRUE), nosim), 
        	1, var)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black") 
g <- g + geom_vline(xintercept = 2.92, size = 2) + xlab("Variance Values")
g + facet_grid(. ~ size)
```

Doing the same thing with 20 and 30 dice gives the same mean ($2.92$), with tighter distributions of sample variances. As sample size increases, the estimate of the population variance (mean of all the sample variances) becomes more precise.

> The average of all the samples' variances is a good estimate of population variance, _if_ the sampling is unbiased (random)

## 0503 Standard error of the mean

Recall some traits about the distributions of sample means

1. If we take a random sample from a population, and calculate the sample's mean value, that average is itself a random variable
2. That random variable has _its own_ population mean and population variance
3. The population distribution of this variable is centered around mean of the _original_ population: $E[\bar X] = \mu$
4. That variable's population variance is $Var({\overline{X}}) = \sigma^2 / n$
	+ where $n$ is the size of a sample
	+ __The variance in a sample decreases linearly as sample size increases__
5. We usually know $n$, can often estimate $\sigma^2$, and measure $Var(\overline{X})$



> The __Standard Error__ of the Mean is the square root of the __variance of the sample mean__: $Var({\overline{X}}) = \sigma^2 / n$
> $${SE_\overline{x}} = {\sigma / \sqrt{n}} \approx {S / \sqrt{n}}$$

Standard Error is the variability of the sample population's average value, and represents the precision of the estimate. As sample size increases, standard error decreases; at infinite samples, the standard error is zero. 

(Any quantity can be estimated and have a standard error - mean, median, variance, regression coefficients, etc.)
	
### Summary of standard error

1. The standard error is the standard deviation of any population _statistic_
2. Standard error is the variability of a sample's average value
3. Standard error converges to the standard deviation $\sigma$ as sample size increases
4. $SE_x = \sqrt{Var(\bar{X})} = {\sqrt{S_{X}^{2}}} = {\sigma/ \sqrt{n}} \approx {S / \sqrt{n}}$ 

1. A sample's variance $S^2$ estimates the population variance $\sigma^2$
3. The distribution of a sample's variance $S^2$ is centered around the "true" value $\sigma^2$
4. That distribution becomes more concentrated as sample size $n$ increases
5. Similarly, the sample mean $\bar{X}$ estimates the population mean $\mu$
5. The sample mean's distribution is centered around $\mu$
6. The sample mean's distribution becomes more concentrated as sample size $n$ increases
7. The variance of a sample converges exactly to $\sigma^2 /n$
	+ with real data, draw many samples, yielding many sample means, to estimate $\sigma^2$
 
	

## 0504 Variance Example
The father/son data set contains height measurements of fathers and their sons.
```{r echo=FALSE, message=FALSE}
library(UsingR); data(father.son);
x <- father.son$sheight
n <- length(x) # num observations
```

Sons' heights look Gaussian. This variance of this plot _estimates_ the height 
variability of whatever population this set came from (caucasian, african, etc.)
```{r}

hist(x, plot=TRUE, breaks=seq(58, 80), probability=TRUE)
lines(density(x), col="red", lwd=3)
```

Values for this plot of childrens' heights

* Population Variance $(\sigma^2)$: `r round(var(x), 2)` 
* Sample Variance $({S^2})$: `r round(var(x)/n, 2)` 
* Standard Deviation $(\sigma)$: `r round(sd(x), 2)` 
* Standard Error $({\sigma}/{\sqrt{n}})$: `r round(sd(x)/sqrt(n), 2)` 

The samples' variance and Standard Error measure the variance and deviation of the
height _averages_ in groups of $n$ children.

### Summary of Variances

0. A sample's variance is a random variable (sample variance) that has its own distribution and mean
1. The distribution of the sample variance $S^2$ is centered at whatever it is estimating
2. The distribution of the sample variance $S^2$ gets more concentrated around 
the population variance $(\sigma^2)$ as $n$ increases
3. The variance of the sample mean: $Var(\overline{X}) = {\sigma^2}/n$ __estimates__ the population variance $(\sigma^2)$
5. The variance of the sample represents the variability of values drawn from a population
6. The standard error represents the variability of sample means, for samples drawn from a population


# 06 Common Distributions

## 0601 Bernoulli & Binomial

### The Bernoulli Process

Any experiment with 2 possible outcomes that has total probability of $1$ is a __Bernoulli trial__.
A sequence of independent Bernoulli trials is a __Bernoulli process__. 
A Bernoulli process always leads to a __Binomial distribution__.

Bernoulli random variables take only the values $1$ and $0$ with respective probabilities __$p$__ and __$(1-p)$__.

The mean and variance of any Bernoulli random variable are always __$p$__ and __$p(1-p)$__

The Bernoulli __probability mass function__ (PMF) is $$P(X=x) = {p^x}(1-p)^{1-x}$$



### The Binomial Distribution

The binomial distribution is the result of a finite number of Bernoulli trials in which the probability
of each outcome remains constant. The binomial distribution is composed of _discrete_ values

* A binomial random variable is the __sum__ of several Bernoulli variables
	+ EX: the total number of heads from coin flips

If $X_n$ are iid $Bernoulli(p)$  (Bernoulli variables), then the binomial random
variable is defined as $$X = \sum_{i=1}^{n}X_i$$

The binomial __probability mass function__ (PMF) is $$P(X=x) = \binom{n}{x}{p^x}(1-p)^{n-x}$$

* $\binom{n}{x}$ is "n choose x", the number of combinations of $n$ and $x$

--- 

#### Example 1: A binomial calculation

A friend has 8 children; 7 girls and 1 boy.

If each gender has an independent 50% probability for each birth, what is the 
probability of getting 7 __or more__ girls out of 8 births?

Two positive conditions: $7$ girls or $8$ girls, therefore the binomial variable $P(X)$
is the sum of the Bernoulli variables $P(7)$ and $P(8)$  

$$P(X) = P(7) + P(8)$$
$$P(X=x) = \binom{n}{x}{p^x}(1-p)^{n-x}$$
$$P(X=7) = \binom{8}{7}{p^7}(1-p)^{8-7} = \binom{8}{7}{.5^7}(1-.5)^{8-7}$$
$$P(X=8) = \binom{8}{8}{.5^8}(1-.5)^{8-8}$$

$$P(7) + P(8) \approx 0.04$$

Code by:
```{r}
# library("combinat");
choose(8,7) * .5^7 * (1-.5)^(8-7) + choose(8,8) * .5^8 * (1-.5)^(8-8)

# simplifies to 
choose(8,7) * .5^8 + choose(8,8) * .5^8

# can also use pbinom *** q > 6 = {7,8}
pbinom(q = 6, size = 8, prob = .5, lower.tail = FALSE)  # lower.tail false means p(X > x)
```


## 0602 The Normal Distribution

The normal distribution is the result of any random process, and is _continuous_.
The normal converges to the Gaussian at $\pm \infty$

A normal distribution with mean $\mu$ and variance $\sigma^2$ has a probability density given by the Gaussian function 

  $$X = \sqrt{2\pi \sigma^2}e^{-(x - \mu)^2/2\sigma^2}$$

---

If a random varibale $X$ has this normal density distribution ($E[X] = \mu$ and $Var(X) = \sigma^2$)
a shorthand notation is
$$X\sim \mbox{N}(\mu, \sigma^2)$$


- When $\mu = 0$ and $\sigma = 1$ the resulting distribution is called the __standard normal distribution__
- _Standard Normal_ random variables are often labeled $Z$

---

The standard normal distribution with reference lines: 
```{r, fig.height=5, fig.width=7, fig.align='center', echo = FALSE}
x <- seq(-3, 3, length = 1000)
library(ggplot2)
g <- ggplot(data.frame(x = x, y = dnorm(x)), 
            aes(x = x, y = y)) + geom_line(size = 1, color = "red")
g <- g + geom_vline(xintercept = -3 : 3, size = 1) + scale_x_continuous( breaks=seq( from=-3, to=3, by=1) )
g
```

---

### Facts about the normal density

If $X$ has a non-standard normal distribution, it can be shifted to a  _standard_ normal distribution
by subtracting $\mu$ and dividing by $\sigma$

$$\mbox{For} ~~ X \sim \mbox{N}(\mu,\sigma^2): ~~ \frac{X -\mu}{\sigma} \sim N(0, 1)$$ 

Also

$$Z = \frac{X -\mu}{\sigma} \sim N(0, 1)$$ 

Then $Z$  is a random variable with a standard normal distribution, and

$$X = \mu + \sigma Z \sim \mbox{N}(\mu, \sigma^2)$$

So, random variables can be shifted to and from standard normal distributions.

---

### More facts about the normal density distribution

1. Approximately $68\%$, $95\%$ and $99\%$  of the normal density lies within $1$, $2$ and $3$ standard deviations from the mean, respectively
2. $-1.28$, $-1.645$, $-1.96$ and $-2.33$ are the $10^{th}$, $5^{th}$, $2.5^{th}$ and $1^{st}$ percentiles of the standard normal distribution respectively
3. By symmetry, $1.28$, $1.645$, $1.96$ and $2.33$ are the $90^{th}$, $95^{th}$, $97.5^{th}$ and $99^{th}$ percentiles of the standard normal distribution respectively
4. For a _standard normal_ distribution, quantiles and probabilities are the same

---

#### Example 1: Deviation of a percentile

How many standard deviations from the mean is the $95^{th}$ percentile of a $N(\mu, \sigma^2)$ distribution? 

* Quick answer in R `qnorm(.95, mean = mu, sd = sigma)`
* Or, because you know that $1.645 \sigma$ is the $95^{th}$ percentile, you can simply add that deviation to the mean $\mu$:
$$\mu + \sigma 1.645$$
* (In general any non-standard quantile is just $\mu + \sigma z_0$ where $z_0$ is the appropriate _standard normal_ quantile)
* Finally, calculating in R for a standard normal distribution:
```{r}
qnorm(p=.95, mean=0, sd=1, )
```


---

#### Example 2: Probability of a value

What is the probability that a $\mbox{N}(\mu,\sigma^2)$ RV is larger than $x$?

* $x$ is in units of standard deviations
* In R, use `pnorm(p = x, mean = mu, sd = sd, lower.tail = FALSE)`
  	+ `lower.tail = TRUE` calculates $P(X â‰¤ x)$ 
  	+ `lower.tail = FALSE` calculates $P(X > x)$
* Or, use `1 - pnorm(p = x, mean = mu, sd = sd, lower.tail = TRUE )`
* If $x$ is in raw units, you can shift to convenient $\mbox{N}(\mu,\sigma^2)$ units via ${x- \mu} \over {\sigma}$

Calculating in R for a standard normal distribution:

```{r}
# If mean = 0, Var = 1, and x = 1.96 sd  ~ 97.5 percentile
pnorm(q=1.96, mean=0, sd=1, lower.tail=FALSE)

# If x = 1.96 sd ~ 97.5 percentile
1 - pnorm(q=1.96, mean=0, sd=1, lower.tail=TRUE)
```


---

#### Example 3: Probability of ad clicks

Assume that the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50. What's the probability of getting more than  1,160 clicks in a day?

It's not very likely, 1,160 is `r (1160 - 1020) / 50` standard
deviations from the mean 
```{r}
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm(2.8, lower.tail = FALSE)
```


---

#### Example 4: Ad click quantiles

Assume that the number of daily ad clicks for a company 
is (approximately) normally distributed with a mean of 1020 and a standard
deviation of 50. What number of daily ad clicks would represent
the one where 75% of days have fewer clicks (assuming
days are independent and identically distributed)?

```{r}
qnorm(0.75, mean = 1020, sd = 50)
```




## 0603 The Poisson Distribution

The Poisson is a special case of the binomial distribution, used to model counts of things or events, often per unit time. The Poisson is the result of an infinite number of Bernoulli trials, each with an infinite number of possible outcomes per trial, AND the probability of each outcome can change between trials.

The Poisson __probability mass function__ for $x=0,1,2,\ldots, \infty$ is 
$$P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}$$

```{r echo=FALSE, fig.align='center'}
ggplot() + aes(x=1:50, y=ppois(1:50,25)) + geom_point() + ggtitle(expression(paste("Poisson(x, ", lambda, " = 25)")))
```
The Poisson distribution is sigmoidal. Raising $\lambda$ shifts the midpoint to the right; lowering $\lambda$ shifts the midpoint to the left.


---

### Summary 
* Used to model counts
* $x$ is discrete, with domain from $0$ to $\infty$
* Both the mean and variance of the Poisson distribution are $\lambda$
* In the real world, mean and variance of Poisson random variables should be equal
	+ Use this property to check the validity of real world data

---

### Some uses for the Poisson distribution
* Modeling count data (especially unbounded counts:  $x \rightarrow \infty$ )
* Modeling event-time or survival data
	+ Ex. number of surviving cells at time T (survival)
	+ Ex. time to re-occurence of a disease after treatment (event-time)
* Modeling contingency tables
	+ Ex. counts of people by a differentiating trait (a classification)
* Approximating binomials when $n$ is large and $p$ is small
	+ many trials, small probability of success
	+ Ex. large population, few disease occurences

---

### Rates and Poisson random variables
* Poisson random variables are also used to model rates
* $X \sim Poisson(\lambda t)$ where 
	+ $\lambda = E[X / t]$ is the expected count per unit of time
	+ $t$ is the total monitoring time
  
---

#### Example 1: People arriving at a bus stop
The number of people that show up at a bus stop is Poisson with a mean of $2.5$ people per hour.

If watching the bus stop for 4 hours, what is the probability that $3$
or fewer people show up for the whole time?

```{r echo=FALSE, fig.align='center', fig.height=2.5, fig.width=3.5}
ggplot() + aes(x=1:4, y=ppois(1:4,10)) + geom_point() +
	   ggtitle(expression(paste("Poisson(X, ", lambda, " = 10)"))) +
	   ylab("P(X < x)") + xlab("x")
```


```{r}
ppois(3, lambda = 2.5 * 4)
```

---

### Poisson approximation to the binomial
When $n$ is large and $p$ is small the Poisson is an accurate approximation to the binomial distribution

Notation 

* $X \sim \mbox{Binomial}(n, p)$
* $\lambda = n p ~~$ or $~~ \lambda = \mbox{trials * probability}$
	  
---

#### Example, Poisson approximation to the binomial

We flip a coin with success probablity $0.01$ five hundred times. 

What's the probability of 2 or fewer successes?

```{r echo=FALSE, fig.align='center', fig.height=2.5}
poisson <- ggplot() + aes(x=1:4, y=ppois(q=1:4, lambda=500 * .01)) + geom_point() + 
	   geom_line(color="red") + ggtitle("Poisson") +
	   ylab("P(X < x)") + xlab("x")

binomial <- ggplot() + aes(x=1:4, y=pbinom(q=1:4, size=500, prob=.01)) + geom_point() + 
	    geom_line(color="red") + ggtitle("Binomial") +
	    ylab("P(X < x)") + xlab("x")

multiplot(binomial, poisson, cols = 2);
```

```{r}
pbinom(2, size = 500, prob = .01)
ppois(2, lambda=500 * .01)
```

#07 Asymptotes


## 0701


## 0702


## 0703

