---
title: "c6w2l3 - Asymptotics"
author: "Robert Handsfield"
date: "10/11/2015"
output:
  html_document:
    highlight: haddock
    number_sections: yes
    toc: yes
---


```{r echo=FALSE}
library("ggplot2")
library("graphics")
library("grDevices")

```


```{r multiplotFunction, echo=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```




# 07 Asymptotics (Limits of random variables)

Asymptotics is the behavior of statistics as the sample size goes to infinities, zeroes, or some other limit. In general we only care about what happens as the number of samples approaches to infinity.

Asymptotics for the basis for moving from discrete to continuous systems, i.e. counts to frequencies.

---

The limiting behavior of distributions engenders 2 methods for working with large samples of iid observations.

1. The Law of Large Numbers
2. The Central Limit Theorem

## 0701 The Law of Large Numbers

This law just says that sample averages converge to the population average as the number of samples increases.

* Remember also that an estimator is ___consistent___ if it converges to what you want to estimate

---

#### As a number of samples grows

* The distribution's average converges to whatever the distribution is estimating
	+ if the samples are independent and identically distributed (___iid___)
	+ often the _sample mean_ of a distribution converging to the true _population mean_ of the universe
* $\therefore$ by the LLN, the _sample mean_ of an iid distribution is consistent for the _population mean_
	+ any good estimator should be consistent
* The consistency property is also true for the sample~population variance and the sample~population standard deviation	

---

#### Example 1: Frequency of a coin flip

The population mean $\bar{X}_{n}$ is the sample proportion of heads, the average result of $n$ coin flips.
As the number of flips $n$ increases, $\bar{X}_{n}$ eventually converges to the true probability of a head, $0.50$.

In this example, _sample mean_ estimates the probability of _Heads_. As samples $n$ increase, it converges to 0.50, meaning that this particular distribution is a ___consistent___ estimator.

```{r fig.align='center', fig.height=3}
set.seed(1234)

# generate 1000 random, normally distributed numbers
n <- 10000
nums <- cumsum(rbinom(n=n, size=1, prob=.5))

# dividing by the vector 1:n gives a vector of 
# the running means of the normal distribution
means <- nums/(1:n)

#pnums <- ggplot() + aes(x=1:n, y=nums) + geom_point(size=1) + ggtitle("Binomial Distribution")
pmeans <- ggplot() + aes(x=1:n, y=means) + geom_point(size=1) + ggtitle("Probability of Heads (Sample Mean)")
pmeans <- pmeans + geom_hline(yintercept=0.5) + geom_line()

pmeans
```

Because the probability of heads converges to $0.5$, this coin flip estimator is consistent.

---

#### Example 2: Large Numbers and the Normal Distribution

The Law of Large Numbers also applies to the normal distribution. As samples $n$ increase, the sample mean converges to zero.
```{r fig.align='center', fig.height=3}
set.seed(1234)
# generate 1000 random, normally distributed numbers
n <- 1000
nums <- cumsum(rnorm(n))

# dividing by the vector 1:n gives a vector of 
# the running means of the normal distribution
means <- nums/(1:n)

pnums <- ggplot() + aes(x=1:n, y=nums) + geom_point(size=1) + ggtitle("Normal Distribution")
pmeans <- ggplot( ) + aes(x=1:n, y=means) + geom_point(size=1) + ggtitle("Sample Mean")

multiplot(pnums, pmeans, cols = 2)

```
Because the mean of our normally-distributed sample converges to zero, this estimator of the normal population is consistent.



## 0702 The Central Limit Theorem

The Central Limit Theorem just says that any distribution of the average values of iid variables (properly normalized) converges to the standard normal as the sample size increases.

In other words, every distribution of random variables approximates a Normal, and converges as the sample size approaches infinity.

In maths, it means if we Z-transform a sample variable by its mean and standard error, it will converge to the Gaussian standard normal as sample size goes to infinity. 

$$\lim\left [ {\frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}} \right ]_{n \to \infty} \to Gauss(x|\mu=0,\sigma=1)$$

$$
\lim_{n \to \infty} \left[ {\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}
= \frac{\sqrt n (\bar X_n - \mu)}{\sigma}} \right ]
= \frac{e^{{-(x-\mu)^2} \over 2\sigma^2}}{\sigma\sqrt{2\pi}}\bigg|_{\mu=0, ~ \sigma=1}
= \frac{e^{{-x^2} \over 2}}{\sqrt{2\pi}}
$$

* The sample standard deviation $SD$ or $S$ can be substituted for the population standard deviation $\sigma$ at any point.

---

Describing this from the _other direction_, we can say that the _sample average_ is normally distributed with mean approximating the population mean $\mu$ and variance approximating the standard error of the population mean ${\sigma^2}/n$, or

> By the CLT: $$\bar X_n \approx N(\mu, ~ {\sigma^2}/{n})$$

---

#### Example 1: CLT and a six-sided die

Simulate a standard normal random variable by rolling $n$ six-sided dice.

For six-sided dice, recall that

* the mean $\mu = E[X_i] = 3.5$
* the variance $Var(X_i) = 2.92$ 
* standard error of the mean $SE = \sqrt{2.92 / n} = 1.71 / \sqrt{n}$

Let $X_i$ be the outcome for die $i$:

1. roll $n$ dice, take their mean, subtract off 3.5,and divide by $1.71 / \sqrt{n}$ 
	+ $(\bar{X_n} - \mu) / SE$
2. and repeat this over and over
3. by the CLT, this should look like a standard normal

```{r, echo = FALSE, fig.height = 3, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71
dat <- data.frame(
  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))

g <- ggplot(dat, aes(x = x, fill = size))
g <- g + geom_histogram(alpha = .20, binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 1, color="red")
g + facet_grid(. ~ size)
```
The approximation improves as we roll 20, then 30 dice 1000 times.

---

#### Example 2: CLT and a coin flip

Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin

Recall that:

* The sample proportion, say $\hat p$, is the average of the coin flips
* $p$ is the probability of a success
	+ in this case that probability $p$ is the __mean of the estimate__
* $E[X_i] = p$ and $Var(X_i) = p(1-p)$
* Standard error of the mean is $SE = \sqrt{p(1-p)/n}$

---

The CLT states that the Z-transformed probability variable should approximate the normal distribution.
$${\frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}} 
= {\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}}
= \frac{\hat p - p}{\sqrt{p(1-p)/n}} \to Gauss(x|\mu=0,\sigma=1)
$$

* Let's flip a coin $n$ times, take the sample proportion $\hat p$ of heads, subtract off $.5$ and multiply the result by $2 \sqrt{n}$ (divide by $1/(2 \sqrt{n})$)

---

Simulation results for a fair coin
```{r, echo = FALSE, fig.height = 3, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5) 
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 1, color="red")
g + facet_grid(. ~ size)
```

---

Simulation results for a biased coin, $p = 0.5$
```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}
nosim <- 1000
cfunc <- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)
dat <- data.frame(
  x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE), 
                     nosim), 1, cfunc, 10),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE), 
                     nosim), 1, cfunc, 20),
        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE), 
                     nosim), 1, cfunc, 30)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = "black", aes(y = ..density..)) 
g <- g + stat_function(fun = dnorm, size = 1, color="red")
g + facet_grid(. ~ size)
```


## 0703 Confidence Intervals

The confidence interval $CI({\bar X}, n)$ is a measure of certainty that any random sample of size $n$, drawn from the population of values for $\bar X$ will contain a value equal to the population mean $\mu$.

The actual interval is always some multiple $K$ of the standard error, where $K$ is the number of standard deviations corresponding to the (standard normal) quantile of whatever confidence level you want. Ex. $95\% \sim 1.96$, $90\% \sim 1.65$, $etc.$ $K$ is often labeled $Z$. When using Standard Normal tables, use the $Z$ value for $K$.  

* Remember that the _standard error_ is just the standard deviation of a sample, which is equal to the population standard deviation divided by the square root of the sample size. 
	+ $SE = \sigma / \sqrt n$

> The general formula for a confidence interval is $$CI({\bar X}, K, n) = {\mu \pm K\sigma / \sqrt n} ~~ = ~~ \mbox{mean} \pm K * SE$$

### Confidence Intervals for the Normal Distribution

For the _normal distribution_, the confidence interval is calculated by $CI({\bar X}, n) = {\mu} \pm K\sigma / \sqrt n$

---

#### Example 1: The 95% confidence interval
Remember that a sample mean $\bar X$ is always normally distributed, and has population mean $\mu$ and population SD $\sigma / \sqrt n$. Due to the properties of the normal distribution, there is a 95% chance that any value of $\bar X$ is within 2 standard deviations of the mean.

$$P({\bar X} \in ~ \mu \pm 2\sigma / \sqrt n) = 95\%$$

We can also flip this around and say that there is a 95% chance that $\mu$ is within a $2 ~ SD$ interval of any value of $\bar X$.

$$P(\mu \in ~ {\bar X} \pm 2\sigma / \sqrt n) = 95\%$$

__For the _normal distribution_, the $95\%$ confidence interval is usually calculated by__ $CI({\bar X}, n) = {\mu} \pm 2\sigma / \sqrt n$

---

$({\bar X} \pm 2\sigma / \sqrt n)$ is called the __$95\%$ interval for $\mu$__, meaning that if  we

1. Take many random samples of size $n$ from ${\bar X}$ 
2. Construct the confidence interval for each random sample
3. Then __~$95\%$ of those confidence intervals would contain a value equal to $\mu$__

$\mu$ can represent any parameter that we're trying to estimate (as long as $\bar X$ is a collection of values for that same parameter).


Technically, the $95\%$ confidence interval is within $1.96$ standard deviations; we round to $2$ for convenience. Similarly, the $90\%$ confidence interval is within $\pm 1.645 ~ SDs$, ~ to the $5^{th}$ and $95^{th}$ percentiles.


#### Example 2: Confidence interval for average height

Using the `father.son` data set in the UsingR package, construct a 95% confidence interval for the average height of sons.

```{r message=FALSE, warning=FALSE}
library("UsingR")
data(father.son)

x <- father.son$sheight
n <- length(x)
stdev <- sd(x)


# the 95% confidence interval in feet; `qnorm` is the K term
ci <- (mean(x) + c(-1,1) * qnorm(0.975) * stdev/sqrt(n))/12 # convert from inches to feet

ci
```
In the preceding example, `c(-1,1)` accomplishes the 'plus or minus' $\pm$ operation.  
The multiplier `qnorm(0.975)` returns $1.96$, the number of standard deviations corresponding to the $0.975$ quantile

* The $95\%$ confidence interval for sons' height is $5.710$ to $5.738$ feet.
* The $90\%$ confidence interval for sons' height is $5.712$ to $5.735$ feet.
* etc.

--- 

### Confidence Intervals for the Binomial Distribution

In the binomial distribution, each $X_i$ is $1$ or $0$ with probabilities $p$ and $1-p$. The distribution's variance is $\sigma^2 = p(1 - p)$.

The binomial confidence interval takes the form
$$CI({\bar X}, K, n) = \mbox{mean} \pm K * SE \approx \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}$$

* still trying to estimate $p$
* $z_{1 - \alpha/2}$ is the _normal quantile_
* $\sqrt{\frac{p(1 - p)}{n}}$ is the standard error
* $\hat p$ is the proportion of successes observed in the sample (estimating the true probability $p$).

---

The quantity we're estimating is $p$, the true probability. This is a problem because $p$ appears in the standard error term of the confidence interval. To solve this we replace $p$ with the _success proportion_ $\hat p$

$$\hat p \pm z_{1 - \alpha/2} \sqrt{\frac{{\hat p}(1 - {\hat p})}{n}}$$

Replacing $p$ by $\hat p$ in the standard error results in a $CI$ called the ___Wald confidence interval___ for $p$.

---

#### Example 3: The 95% Wald confidence interval

For 95% intervals only, the Wald confidence of $p$ can very simply approximated by 

$$CI({\bar X}, K, n) \to W(\hat p, K|_.95, n)  \le \hat p \pm \frac{1}{\sqrt{n}}$$ 

Even more simply, the domain of the Wald interval is just $$\frac{1}{\sqrt{n}}$$


#### Example 3a: Election predictions

Your campaign manager tells you that in a random sample of 100 likely voters, 56 intend to vote for you.

From these results

1. Do you expect to win?
2. How precise is this estimate?

---

__Question 1__
From the sample, you should receive $56\%$ of the vote. What is the $CI$ of this estimate?  

Approximating with the $95\%$ Wald interval

$$SE(W|_.95) \le \frac{1}{\sqrt n} = 1/\sqrt{100} = 0.1 = 10\%$$

From $CI_{binom} = \hat p \pm SE_W$, you can expect $56\% \pm 10\%$ of the votes, or between $46\%$ and $66\%$

```{r}
0.56 + c(-1,1)*1/sqrt(100)
```
__0.46 to 0.66__

Doing the full calculation $\hat p \pm K * \sqrt{\frac{p(1-p)}{n}}$
```{r}
# K is the qnorm function
0.56 + c(-1,1) * qnorm(0.975) * sqrt(0.56 * 0.44/100)
```

__0.463 to 0.657__  This checks out

You can also use the R `binom` function
```{r}
binom.test(56,100)$conf.int
```
__0.457 to 0.659__  This checks out too


"Can't rule out results less than 50% with 95% confidence"  
OR  
"Can't be 95% confident that you'll receive more than 50% of votes"  
OR  

Dividing the $20\%$ interval into 5 parts of $4\%$ shows that based on this estimate, you expect only an $80\%$ probability of winning??

> This example illustrates the general point that in binomial experiments, you need at least 100 samples for 1 decimal place in your interval, 10K for 2, and 1M for 3.

---

__Question 2__

With $100$ samples, ${1 \over \sqrt{100}} = 0.10$; this estimate is precise to 1 decimal place, or 10%.

---

#### Example 4: Wald interval of a simulated coin flip 

Over 1000 simulations, how often does the Wald interval cover the true coin probability?

Calculate the 95% CI for simulated coin flips
```{r}
n <- 20;  # flip 20 coins
nosim <- 1000;  # 1000 times
pvals <- seq(.1, .9, by = .05);  # do it for 20 different probabilities
				 # between 0.1 and 0.9

# loop over all the probabilities; for each one,
# flip 20 coins 1000 times; then,
# find the lower limit and upper limit for the CI
coverage <- sapply(pvals, function(p){
  phats <- rbinom(nosim, prob = p, size = n) / n
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)  # lower limit
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)  # upper limit
  mean(ll < p & ul > p)  # proportion that true P is within CI limits
})
```

---

__Plot of the results (not so good)__

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=6}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)
````

When the true probability is 0.5, our calculated 95% CI bounds the value 0.5 about 97% of the time (better than 95%). For any other probability (doped coins), the 95% CI does not include the true probability 95% of the time. 

This is because $n = 1000$ is not large enough for the central limit theory to apply accurately to all the values of $p$. 

If we're stuck with small $n$ we can compensate by compressing our success proportion $\hat p$ towards the value $0.5$. This transforms our Wald interval into an Agresti/Coull interval, and it's done like this:

1. Add 2 successes to your vector of heads/tails observations ($X$)
2. Add 2 failures to your vector of observations ($X$)
3. Add 4 to your $n$

Normally, the success proportion $\hat p = \frac{X}{n}$
After steps 1-3, our success proportion looks like this $\hat p = \frac{X+2}{n+4}$

4. Now calculate $CI = \hat p \pm z_{1 - \alpha/2} \sqrt{\frac{{\hat p}(1 - {\hat p})}{n}}$ normally.

---

Simulation with the Agresti/Coull interval:  $n=20$ but added 2 successes and failures
```{r}
n <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000
coverage <- sapply(pvals, function(p){
  phats <- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)
  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)
  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)
  mean(ll < p & ul > p)
})
```

__These results look better__
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)
````

---

### The Poisson interval

Confidence intervals for the Poisson distribution follow the general form

> $$CI_{Poisson} = \mu \pm K * SE = Estimate \pm Z-quantile * SE_{est}$$

$Z-quantile$ is the _normal quantile_

#### Example 5: Nuclear pump failure rate

A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the ___failure rate___ per day.

* Assume that number of failures $X$ is Poisson-distributed
	+ $X \sim Poisson(\lambda t)$
* Estimate the _rate_ $\lambda$ of failure ($t$ in days)
	+ $\hat \lambda = X/t$
	+ $\hat \lambda$ is the _empirical rate_
* The variance equals the mean (by Poisson properties)
	+ $Var(\hat \lambda) = \lambda / t$ 
	+ (notice $\lambda$ vs $\hat \lambda$) 
* $\hat \lambda / t$ is our __empirical variance estimate__

---

In R
```{r}
x <- 5; t <- 94.32; lambda <- x / t

# via calculation
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)

# via R function
poisson.test(x, T = 94.32)$conf
```
The `poisson` function gives 'exact' CI, but that interval tends to be conservative, i.e. the interval is wider than it needs to be to achieve 95% coverage.

--- 

#### Example 6: Simulating the Poisson coverage rate 

Let's see how this interval performs for lambda values near what we're estimating
```{r}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 100
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
```

(Gets really bad for small values of lambda)
```{r, fig.align='center', fig.height=4, echo=FALSE}
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0) + ggtitle("Percent of time the CI included Lambda")
```` 
As $\lambda$ gets larger, coverage gets better.

---

What if we increase t to 1000?
```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}
lambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000
t <- 1000
coverage <- sapply(lambdavals, function(lambda){
  lhats <- rpois(nosim, lambda = lambda * t) / t
  ll <- lhats - qnorm(.975) * sqrt(lhats / t)
  ul <- lhats + qnorm(.975) * sqrt(lhats / t)
  mean(ll < lambda & ul > lambda)
})
ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0) + ggtitle("Percent of time the CI included Lambda")
```
Coverage converges to $95\%$

---

## Summary
* The LLN states that averages of iid samples converge to the population means that they are estimating
* The CLT states that averages are approximately normal, with distributions
* centered at the population mean 
* with standard deviation equal to the standard error of the mean
* CLT gives no guarantee that $n$ is large enough
* Taking the mean and adding and subtracting the relevant normal quantile times the SE yields a confidence interval for the mean 
* Adding and subtracting 2 SEs works for 95% intervals
* Confidence intervals get wider as the coverage increases (why?)
* Confidence intervals get narrower with less variability or larger sample sizes
* The Poisson and binomial case have exact intervals that don't require the CLT
* But a quick fix for small sample size binomial calculations is to add 2 successes and failures





























