---
title: "c6w3l1"
author: "R. Handsfield"
date: "January 11, 2016"
output:
  html_document:
    number_sections: yes
    toc: yes
#  pdf_document:
#    number_sections: yes
#    toc: yes
---

# 08 Confidence Intervals

In previous lectures, we calculated confidence intervals for large samples. These confidence intervals were based on the Central Limit Theorem, which becomes more accurate as the sample size grows to infinity. The CLT method is not accurate for small samples. Instead, we use a __T-Confidence Interval__. The _T-Interval_ is designed to give accurate estimates for small populations. The _T-Interval_ is also used when the population standard deviation $\sigma$ is unknown.

> __Rule of Thumb__
> * Use a _T-Interval_ for populations less than 30, or when $\sigma$ is unknown
> * Use a CLT Interval for populations greater than 30

## 0801 T-Confidence Intervals

The _T-Interval_, or _Gosset's Interval_, is used to determine the confidence of estimates of small populations, or when the population standard deviation $\sigma$ is unknown.

Recall that the general form of a confidence interval is $$\mbox{estimate} \pm K * SE$$ 

* $\mbox{estimate}$ is a number
* $K$ is the number of standard deviations corresponding to a standard normal quantile 
	+ (95% confidence, 90% confidence, etc.)
* $SE$ is the standard error of the estimate

The CLT Confidence Interval takes the canonical form 
$$CI({\bar X}, K, n) = Est \pm ZQ \times SE_{Est} = {\mu \pm K \frac{\sigma}{\sqrt n} }$$

* $K$ = the _Z-Quantile_ $ZQ$, a number of standard deviations of the ___Standard Normal Distribution___ 

Instead of the Z-Quantile, the _T-Confidence Interval_ uses a $K$ multiplier equal to the _T-Quantile_ $TQ$, which is also a number of standard deviations, but taken from the ___T-Distribution___.

The T-Distribution looks similar to the normal distribution, but is shorter and wider with fatter tails, meaning the data is more spread out and the variance is higher. This in turn makes the T-Intervals generally wider than their CLT-Interval cousins. As the sample size goes to infinity, the T-Distribution converges to the Standard Distribution, so it's always safe to use the T-Distribution. We'll first look at the T-Confidence Interval for non-independant groups.

The T-Confidence Interval takes the canonical form 
$$CI_t({\bar X}, t, n) = Est \pm TQ \times SE_{Est} = \bar X \pm t_{n-1} \frac{S}{\sqrt n}$$

* $\bar X$ is the estimate
* $t_{n-1}$ is the relevant quantile from the T-Distribution
* $S$ is the standard deviation of the sample
* $n$ is the number of samples

### Skewed Distributions

The t-interval assumes that the data are _iid_ and ___normal___. Data drawn from skewed distributions implicitly violate the second assumption. In these skewed cases, centering a confidence interval at the mean doesn't work. 

Instead, consider

1. Taking logs of data to force the skew into a normalish, non-skewed distribution
2. Using the median as the center point: $CI_t({med}, t, n) = med \pm t_{n-1} \frac{S}{\sqrt n}$

### Discrete Distributions

The t-interval also doesn't work well for discrete data, particularly binary. In discrete data cases, other intervals are available.

@TODO finish section

### 0802 T-Confidence Interval Example

The T-Interval is particularly good at finding confidence intervals for groups of data. This example looks at 10 peoples' sleep duration before and after taking a soporific drug. Because there are two measurements of the the same population, the measurement groups are __NOT__ independant. The groups are ___paired___, so we should use the __paired t-test__.

```{r}
data(sleep)
head(sleep)
```

The groups are _before_ and _after_ (duh), demarked by `sleep$group`. Differentiate them in `ggplot` by setting `aes(x = group)`. You also need to indicate that each group has several measurements with `aes( group = factor(ID) )`.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
```

```{r, fig.width=6, fig.height=6, fig.align='center'}
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
g
```

Results
```{r, echo=TRUE}
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
difference <- g2 - g1
mn <- mean(difference); s <- sd(difference); n <- 10
```
```{r}
difference
```
```{r echo=FALSE}
paste("Mean =", mn, "  SD =", s)
```
Recall that we're considering the sleep changes for 6 people. In our data, this means that the data itself is _paired_ across groups. If we do a _paired t-test_, we acknowledge this, and we are calculating the variations across measurements for individual subjects (subject variations are low because measurements for a single person are all strongly correlated). If we did an unpaired t-test, we would be calculating the variation for all of group 1 with the variation for all of group 2.

The following code shows 4 different ways to obtaining the t-confidence interval. The first is the manual calculation, the other 3 are calls to `t.test()` with different arguments. (Note that in method 2, `t.test(x=difference, ..., paired=FALSE)`. this is ok because `difference` is a vector of the 10 measurement difference between groups; the group pairing is implicitly preserved in `difference`.)

`t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"), mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...)`
```{r, echo = TRUE}
a <- rbind(
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n),
as.vector(t.test(difference)$conf.int),
as.vector(t.test(g2, g1, paired = TRUE)$conf.int),
as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)
)

a[,1] <- signif(a[,1], 4)
a[,2] <- signif(a[,2], 3)
a
```
The interpretation here is that we have 95% confidence that the true population mean is between $0.7$ and $2.46$. But, that can't be right because this sample wasn't drawn from a larger population! The correct interpretation involves the mean increase in sleep time that we calculated earlier. Recall that $\mbox{mean} = 1.58$. Since the mean is within the 95% t-confidence interval, the correct interpretation is:

> If we ran this exact experiment many times, and calculated each 95% CI, 95% of those intervals would contain the true sample mean that we're trying to estimate.

##  0803 T-Intervals for Independent Groups 

In the previous example, we found confidence intervals for non-independant, paired groups. Now we look at _finding t-confidence intervals for truly independant groups_. (This is also known as __A/B Testing__.)

Recognize independant groups by

1. Random samples drawn from a population
2. Differnet sample sizes


Remember that the general form for a CI is
$$CI = \mbox{estimate} \pm \mbox{interval} ~ \rightarrow ~ \mbox{mean} \pm \mbox{Quantile} * \mbox{standard error}$$

The t-confidence interval expresses this form as 
$$\mbox{Paired} ~~ CI_t(\bar X, t, n) = \bar X \pm TQ * \frac{S}{\sqrt n} = \bar X \pm t_{n-1} \frac{S}{\sqrt n}$$

The standard $CI_t$ for groups of pairs is simple because it can treat the _differences_ between the pairs as 1 measurement, and mimic the $CI_{CLT}$.

The $CI_t$ for independant groups is immediately more complicated because it deals with two estimates from two samples (with equal variances). Heres how to do it:

1. Replace the sample mean $\bar X$ with the difference between group averages $\bar Y - \bar X$
2. Use the T-Quantile $t_{n_x + n_y - 2, ~ 1 - \alpha/2}$
	+ $n_x$ and $n_y$ are the numbers of observations in each group
3. Replace $SE_{Est}$ with the _Standard Error of the difference_ $SE_{Dif}$
	+ $SE_{Dif} = S_p *\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}$
	+ $S^2_p = \frac{(n_x - 1) S_x^2 + (n_y - 1) S_y^2}{n_x + n_y -2}$ the _pooled variance_
		+ subtract $2$ from $(n_x + n_y)$ because there are two groups
		+ the pooled variance estimate weights the group variances based on their sample sizes, automatically accounting for different sample sizes, but __NOT__ unequal variances
		
The Result is 
$$
\mbox{Independent} ~~ CI_t(\bar X, \bar Y, t, n_x, n_y) = 
(\bar Y - \bar X) \pm t_{n_x + n_y - 2, ~ 1 - \alpha/2} * S_p *\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}
$$
---

In confusing lecturer form:

- Therefore a $(1 - \alpha)\times 100\%$ confidence interval for $\mu_y - \mu_x$ is 
$$
    \bar Y - \bar X \pm t_{n_x + n_y - 2, ~ 1 - \alpha/2} * S_p *\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}
$$
- The pooled variance estimator is $$S_p^2 = \{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\}/(n_x + n_y - 2)$$ 
- Remember this interval is assuming a constant variance across the two groups  
- If there is some doubt, assume a different variance per group, which we will discuss later

---

### Example - CI for small, independant groups

An experiment measures the systolic blood pressure for 8 _oral contraceptive_ users vs 21 _control_ subjects

- Comparing SBP for 8 oral contraceptive users versus 21 controls
- $\bar X_{OC} = 132.86$ mmHg with $s_{OC} = 15.34$ mmHg
- $\bar X_{C} = 127.44$ mmHg with $s_{C} = 18.23$ mmHg
- Pooled variance estimate $S_p^2 = \{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\}/(n_x + n_y - 2)$

Calculate the $CI_t$ manually. (Note `t.test()` takes vectors of measurements, so we can't use it if the group averages are already calculated)
```{r}

Xoc <- 132.86; Soc <- 15.34; Noc <- 8;
Xc  <- 127.44; Sc  <- 18.23; Nc  <- 21;

# degrees of freedom
dof = Noc + Nc -2

# Pooled standard deiation
Sp <- sqrt( ( (Noc-1) * Soc^2 + (Nc-1) * Sc^2) / dof )

# t-confidence interval for difference between independant groups
tCI <- (Xoc - Xc) + c(-1, 1) * qt(.975, dof ) * Sp * (1/Noc + 1/Nc)^.5
round(tCI,3)
```
This confidence interval represents the range that the blood pressure difference is probably within. But, this interval includes 0! This means there could be no difference in the blood pressure between groups. This is important, so pay attention dammit!

### T-Confidence Sleep Interval Example Revisited

Previously, we calculated the 95% t-confidence interval for difference in sleep hours in people before and after taking a soporific. We treated the data (correctly) as 10 independant pairs of measurements.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
data(sleep)
head(sleep)
```

```{r}
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
T <- t.test(g2, g1, paired = TRUE)  ## paired = TRUE
round(T$conf.int[1:2],4)
```

What happens if we mistakenly treat the data as independent groups? That is, 2 independant groups of 10 measurements? Well, in this case, mathematically, each group of 10 gets averaged to 1 number, and the difference between those averages becomes our Estimator.
```{r}
a <- rbind(
t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf,  # data as 2 groups of 10
t.test(g2, g1, paired = TRUE)$conf  # data as 10 pairs of 2
)

signif(a,4)
```
So, yeah, those look different. Treating the data (wrongly) as 2 groups of 10 measurements (`paired = FALSE`) gives us larger intervals because variance in each group is higher than variance in any pair.

---

### Example `ChickWeight` data in R

Four groups of chicks receive different diets, and each chick has their bodyweight measured at semi-regular intervals. Each group has 10-20 chicks.
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(datasets); library(reshape2); library(dplyr);

data(ChickWeight);
head(ChickWeight);
```

Plotting chick-weight over time looks like this 
```{r, fig.align='center', fig.width=12, fig.height=6}
g <- ggplot(ChickWeight, aes(x = Time, y = weight, colour = Diet, group = Chick));
g <- g + geom_line();
g <- g + stat_summary(aes(group = 1), geom = "line", fun.y = mean, size = 1, col = "black");
g <- g + facet_grid(. ~ Diet);
g <- g + ggtitle("Chick Weight Gain for 4 Diets") + theme(plot.title=element_text(size=24));
g
```
<p style="text-align: center;"> The black line is the average weight for each group. </p>

---

We can estimate visually that _Diet 1_ showed the lowest gains. Plotting the total _gain_ in weight ($\Delta w$) by diet with a violin plot looks like this 

```{r, echo=FALSE, fig.align='center', fig.width=6, fig.height=6, warning=FALSE}
##define weight gain or loss
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight");
names(wideCW)[-(1 : 2)] <- paste("time", names(wideCW)[-(1 : 2)], sep = "");

wideCW <- mutate(wideCW,  gain = time21 - time0);

g <- ggplot(wideCW, aes(x = factor(Diet), y = gain, fill = factor(Diet)))
g <- g + geom_violin(col = "black", size = 1)
g
```
The "height" of each violin reflects the variance of each diet group. In this plot, it's easier to see that most chicks on Diet 1 gained between 100 and 175 units. Most of group 3 gained between 225 and 275.

What is the t-confidence interval for the difference between groups 1 and 4?

The following code asks 

> "If we subtract $(\mbox{Gain}_1 - \mbox{Gain}_4)$, how certain are we that the difference is the actual weight gain difference that would be measured in a larger population of chicks?"
```{r}
wideCW14 <- subset(wideCW, Diet %in% c(1, 4)) # subset diets 1 and 4: in order gain1 - gain4
a <- rbind(
	t.test(gain ~ Diet, paired = FALSE, var.equal = TRUE, data = wideCW14)$conf,
	t.test(gain ~ Diet, paired = FALSE, var.equal = FALSE, data = wideCW14)$conf
)

signif(a,4);
```
> We are 95% confident that chicks on Diet 1 will gain an average of (18 to 104) fewer units than chicks on Diet 4, assuming unequal variances in the sample groups.

Note that assuming whether the group variances are equal makes a difference, a roughly $8.5 \%$ narrower interval. From visual inspection, `var.equal = FALSE` is probably the safer assumption, and the larger values in row 2 are the better interval.

---

## 0804 Unequal variances

Recall that, for groups with similar/equal variances, the _Standard Error of the Difference_ is
$$SE_{Dif} = S_p *\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}$$

Under unequal variances the Standard Error of the Difference is
$$SE_{Dif} = \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}$$

and the t-confidence interval is

$$
\mbox{Independent} ~~ CI_t(\bar X, \bar Y, t, n_x, n_y) = 
(\bar Y - \bar X) \pm * t_{df} * \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}
$$

where $t_{df}$ is calculated with degrees of freedom
$$
df= \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2}
    {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) +
      \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)}
$$

> When in doubt, assume unequal variances

---

### Example

- Comparing SBP for 8 oral contraceptive users versus 21 controls
- $\bar X_{OC} = 132.86$ mmHg with $s_{OC} = 15.34$ mmHg
- $\bar X_{C} = 127.44$ mmHg with $s_{C} = 18.23$ mmHg
- $df=15.04$, $t_{15.04, .975} = 2.13$
- Interval
$$
132.86 - 127.44 \pm 2.13 \left(\frac{15.34^2}{8} + \frac{18.23^2}{21} \right)^{1/2}
= [-8.91, 19.75]
$$
- In R, `t.test(..., var.equal = FALSE)`

---

### Comparing other kinds of data
* For binomial data, there's lots of ways to compare two groups
  * Relative risk, risk difference, odds ratio.
  * Chi-squared tests, normal approximations, exact tests.
* For count data, there's also Chi-squared tests and exact tests.
* We'll leave the discussions for comparing groups of data for binary
  and count data until covering glms in the regression class.
* In addition, Mathematical Biostatistics Boot Camp 2 covers many special
  cases relevant to biostatistics.


