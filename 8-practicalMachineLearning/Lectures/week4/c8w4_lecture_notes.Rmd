---
title: "c8w4_lecture_notes"
author: "R. Handsfield"
date: "September 14, 2016"
output: 
  html_document: 
    highlight: pygments
    number_sections: yes
    theme: cerulean
    toc: yes
---

```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = 'center', fig.height=3, fig.width=5)
```

```{r echo=FALSE, message=FALSE}
library(caret)
library(ggplot2); library(gridExtra)
```

[https://github.com/jtleek](https://github.com/jtleek)

# Regularized Regression

Regularization can reduce a model's variance and combat overfitting. It does this by adding a penalty term $\lambda$ to the cost function, which raises the cost (prediction error) of a specific set of observations $X^{(i)}$; this in turn lowers the magnitude of the parameters $\beta^{(i)}$, resulting in weaker contribution from $X^{(i)}$

Regularized nethods in `caret` are `(lasso, ridge, relaxo)`

If features $x_1$ and $x_2$ are highly correlated, then
$$
\begin{matrix}
Y & = & \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \\ 
 & \approx & \beta_0 + (\beta_1 + \beta_2) x_1 + \epsilon
\end{matrix}
$$

The approximation of $Y$ is

1. pretty close to the actual value of $Y$
2. biased to $x_1$
3. lower in variance (cannot overfit to $x_2$)

### Example Prostate Cancer

Why would we want to reduce the variance of a model? Look at what happens when we train a predictive model with increasing numbers of features.

```{r message=FALSE}
library(ElemStatLearn)
data(prostate)

str(prostate)
```

Train a linear model using the sum of squared residuals for the cost metric.
```{r cache=TRUE}

covnames <- names(prostate[-(9:10)])
y <- prostate$lpsa
x <- prostate[,covnames]

# pseudo-train linear model 
form <- as.formula(paste("lpsa~", paste(covnames, collapse="+"), sep=""))
summary(lm(form, data=prostate[prostate$train,]))

# create training/test sets
set.seed(1)
train.ind <- sample(nrow(prostate), ceiling(nrow(prostate))/2)
y.test <- prostate$lpsa[-train.ind];  x.test <- x[-train.ind,]
y <- prostate$lpsa[train.ind];  x <- x[train.ind,]

p <- length(covnames)
rss <- list()

# calculate the accuracy (metric = sum of squared residuals) of the model as num of features increase
for (i in 1:p) {
  
  # for each i create a 2 x pCi dim array of X column (feature) combinations
  Index <- combn(p,i)

  # rss: row1 = trn SSE; row2 = tst SSE
  rss[[i]] <- apply(Index, 2, function(is) {
    form <- as.formula(paste("y~", paste(covnames[is], collapse="+"), sep=""))
    isfit <- lm(form, data=x)
    yhat <- predict(isfit)
    train.rss <- sum((y - yhat)^2)

    yhat <- predict(isfit, newdata=x.test)
    test.rss <- sum((y.test - yhat)^2)
    c(train.rss, test.rss)
  })
}
```

Plot performance of the linear regression models fir increasing number of features
```{r fig.height=5, fig.width=7}
### PLOTTING CODE CLUSTERFUCKERY ###
# create dummy axes of appropriate space;  `type="n"` means no-plot 
plot(1:p, 1:p, type="n", ylim=range(unlist(rss)), xlim=c(0,p), xlab="number of predictors", ylab="residual sum of squares", main="Prostate cancer data")

# modify trn & tst data for plotting
for (i in 1:p) {
  # x-vec = num features (left shifted);  y = train SSE	
  points( rep( i-0.15, ncol(rss[[i]]) ), rss[[i]][1, ], col="blue")
	
  # x-vec = num features (right shifted); y = test SSE		
  points( rep( i+0.15, ncol(rss[[i]]) ), rss[[i]][2, ], col="red")
}

# get min trn SSE for each num of features
minrss <- sapply(rss, function(x) min(x[1,]))
# plot min trn SSE
lines((1:p)-0.15, minrss, col="blue", lwd=1.7)

# get min tst SSE for each num of features
minrss <- sapply(rss, function(x) min(x[2,]))
# plot min tst SSE
lines((1:p)+0.15, minrss, col="red", lwd=1.7)

legend("topright", c("Train", "Test"), col=c("blue", "red"), pch=1)
```

Note that the training error always decreases, but the test error minimizes at 2-3 features.
This is because all models always overfit the training set to some degree. For this reason, we must always _judge_ models on a validation set, and _report_ a single model's performance on the test set.

## Overcoming Limitations

If the data is limited, bootstrap it. If the model is very complex, train on small subsets of a training set, then bootstrap those models.

---

Another way to overcome computational limits is to decompose the expected prediction error.

Assume that $Y$ can be predicted from some function of $X$, plus an error term:
$$Y_i = f(X_i) + \epsilon_i$$

Then the __Expected Prediction Error__ (MSE) can be expressed as the expectation value of the squared residuals. $\hat f_\lambda$ is the _estimating function_
$$EPE(\lambda) = E \left[ \{ Y - \hat f_\lambda (x^{(i)}) \}^2 \right]$$

The EPE can be _decomposed_ into the __Irreducible Error__, __Squared Bias__, and __Variance__ of the estimate.
$$
EPE ~=~ \sigma^2 ~+~ \{ E[\hat f_\lambda(x^{(i)})] - y^{(i)} \}^2 ~+~ var[\hat f_\lambda (x_0)]
$$

The bias and variance terms can be traded off to achieve best performance.

### Example - Small Training Set

Another issue for high-dimensional data is not having enough training examples. Below, we try to train a model with 8 features, but only 5 training examples.
```{r}
small = prostate[1:5,]
lm(lpsa ~ .,data =small)
```

We could only solve for parameters (weights) for 4 of the 8 features. The rest are NA.
If we tried to solve this model via the normal equation, we would find that the design matrix can't be inverted.

One way to solve this problem is with __Hard Thresholding__:

1. Decide that only some (4) features will be nonzero
2. Set all other feature parameters to zero
3. Train all possible combinations of arbitrarily nonzero features
4. Compare models on a validation set

Step 3 is a computational bitch. It's much easier to effectively reduce the number of features by penalizing all the feature parameters with a __Regularization Parameter__ $\lambda$.

$$
Cost(\beta) = \sum^m_{i=1}{ \left( y^{(i)} - \beta_0 + 
\sum^n_{j=1}{ \beta_j X^{(i)}_j } \right)}^2
 +~ \lambda \sum^n_{j=1}{ \beta^2_j }
$$

This reduces variance and complexity while preserving the structure of the problem. However, including $\lambda$ makes the problem non-singular even if $X^TX$ is not invertible.

Adding regularization turns linear regression into __Ridge Regression__.

### Example - Ridge Regression

```{r}
# ridge regression on prostate dataset
library(MASS)
lambdas <- seq(0,50,len=10)
M <- length(lambdas)
train.rss <- rep(0,M)
test.rss <- rep(0,M)
betas <- matrix(0,ncol(x),M)
for(i in 1:M){
  Formula <-as.formula(paste("y~",paste(covnames,collapse="+"),sep=""))
  fit1 <- lm.ridge(Formula,data=x,lambda=lambdas[i])
  betas[,i] <- fit1$coef
  
  scaledX <- sweep(as.matrix(x),2,fit1$xm)
  scaledX <- sweep(scaledX,2,fit1$scale,"/")
  yhat <- scaledX%*%fit1$coef+fit1$ym
  train.rss[i] <- sum((y - yhat)^2)
  
  scaledX <- sweep(as.matrix(x.test),2,fit1$xm)
  scaledX <- sweep(scaledX,2,fit1$scale,"/")
  yhat <- scaledX%*%fit1$coef+fit1$ym
  test.rss[i] <- sum((y.test - yhat)^2)
}
```

```{r fig.height=5, fig.width=7}
# plot(lambdas,test.rss,type="l",col="red",lwd=2,ylab="RSS",ylim=range(train.rss,test.rss))
# lines(lambdas,train.rss,col="blue",lwd=2,lty=2)
# best.lambda <- lambdas[which.min(test.rss)]
# abline(v=best.lambda+1/9)
# legend(30,30,c("Train","Test"),col=c("blue","red"),lty=c(2,1))

# Ridge Coefficient Paths
plot(lambdas,betas[1,],ylim=range(betas),type="n",ylab="Coefficients", xlab="Lambda")
for(i in 1:ncol(x))
  lines(lambdas,betas[i,],type="b",lty=i,pch=as.character(i))
abline(h=0)
legend("topright",covnames,pch=as.character(1:8))
```

As $lambda$ increases, the magnitudes of the feature parameters converge towards zero.

### Example - Lasso Regression

Lasso Regression is the same as Ridge, but uses a slightly different form for the regularization term, the absolute value of parameters instead of the square:

$$
Cost(\beta) = \sum^m_{i=1}{ \left( y^{(i)} - \beta_0 + 
\sum^n_{j=1}{ \beta_j X^{(i)}_j } \right)}^2
 +~ \lambda \sum^n_{j=1}{ \left| \beta_j \right| }
$$


```{r fig.height=5, fig.width=7, message=FALSE}
# lasso
library(lars)
lasso.fit <- lars(as.matrix(x), y, type="lasso", trace=TRUE)

plot(lasso.fit, breaks=FALSE)
legend("topleft", covnames, pch=8, lty=1:length(covnames), col=1:length(covnames))

# this plots the cross validation curve
lasso.cv <- cv.lars(as.matrix(x), y, K=10, type="lasso", trace=TRUE)
```



# Combining Predictors aka Ensemble Methods

One way to improve a model, is by designing several predictors that each predict one aspect of the model, then combining these predictors by averaging, voting, etc. This improves accuracy but reduces interpretability. Bagging, boosting, and random forests are examples of this method.

Bagging/ boosting/ random forest can combine __similar classifiers__. Model _ensembling_ and _stacking_ can combine __dissimilar classifiers__.



### Example: Wage Data


```{r}
library(ISLR); library(ggplot2); library(caret)
data(Wage)
Wage <- subset(x=Wage, select= -c(logwage))


inBuild <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
buildData <- Wage[inBuild, ]; validation <- Wage[-inBuild, ]

inTrain <- createDataPartition(y = buildData$wage, p = 0.7, list = FALSE)
training <- buildData[inTrain, ]; testing <- buildData[-inTrain, ]

dim(training)
dim(testing)
dim(validation)
```

---

Train 2 models
```{r cache=TRUE, warning=FALSE, message=FALSE}
library(randomForest)

# linear model
mod1 <- train(form = wage~., method="glm", data=training)

# random forest
tr <- trainControl(method = "cv", number = 3)
mod2 <- train(form = wage~., method="rf", data=training, trainControl=tr)
```

---

Make predictions with the models
```{r warning=FALSE, message=FALSE}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)

ggplot(data=testing) + aes(x=pred1, y=pred2, color=wage) + geom_point()
```
Wage is represented by color of the dots. Neither predicter corelates perfectly.
---

Create a model that combines `pred1` and `pred2`
```{r cache=TRUE}
predDF <- data.frame(pred1, pred2, wage=testing$wage)

head(predDF, 3L)

combModFit <- train(form = wage~., method = "glm", data = predDF)

combPred <- predict(object = combModFit, newdata = predDF)
```

---

Compare prediction errors
```{r}
# root sum of squared residuals
rsr <- function(prediction) {
	sqrt( sum( (prediction - testing$wage)^2 ) ); 
}

rsr(pred1); rsr(pred2); rsr(combPred)
```
The combined predictor has the highest accuracy

---

Make some predictions on the validation set
```{r warning=FALSE}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)

predVDF <- data.frame(pred1=pred1V, pred2=pred2V)

combPredV <- predict(combModFit, predVDF)
```

Evaluate the models on the validation set
```{r warning=FALSE}
# use the root sum of squared residuals
rsr <- function(prediction) {
	sqrt( sum( (prediction - testing$wage)^2 ) ); 
}

rsr(pred1V); rsr(pred2V); rsr(combPredV)
```
Hmm, the combination predictor does not perform as well on the validation set.

Some tuning would probably improve this model on the validation set. Anyhoo, you can find ensemble methods in `caretEnsemble` (research this).

The simplified classification prodedure is:

1. Build an _odd_ number of models 
2. Make predictions with each model
3. Predict the final class by majority vote


# Unsupervised Prediction

Unsupervised prediction problems are almost always classification problems. In these problems, we don't even know what the classes are, but we can automatically separate the training data into clusters, then derive classes from those clusters.

The procedure for building cluster predictors is

1. Create some clusters
2. Name the clusters
3. Build a predictor for those clusters


### Example: k-means clustering

```{r}
library(ggplot2)
data(iris)

inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]

dim(training); dim(testing)
```

Do the clustering
```{r cache=TRUE}
ss <- subset(x = training, select = -c(Species))
kMeans1 <- kmeans(x = ss, centers = 3)

training$clusters <- as.factor(kMeans1$cluster)

ggplot(data=training) + aes(x=Petal.Width, y=Petal.Length, color=clusters) + geom_point()
```

Compare the predicted classification to the actual label
```{r}
table(kMeans1$cluster, training$Species)
```

### Example: Binary Tree Classifier

Train classifier
```{r cache=TRUE, message=FALSE}
ss <- subset(x = training, select = -c(Species))
modFit <- train(form = clusters~., method = "rpart", data = ss)

table( predict(modFit, training), training$Species )
```
This classifier is good at prediction setosa and versicolor, but middling at predicting virginica.


Apply classifier to test set
```{r message=FALSE}
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species)
```


# Forecasting

Forecasting uses sequential data to predict future values. It's often performed on time series.
The data are time dependent, and often display distinct temporal patterns:

* _Trend_ - a long term increase or decrease
* _Seasonal Pattern_ - short term pattern related to time of week, month, year, etc.
* _Cycle_ - periodic pattern that repeats

Because of the time dependencies, separating into training/test sets is complicated.

Spatial data exhibits many of the same traits as temporal data, and is subject to the same complexities:

1. Data can depend on nearby observations
2. There may be location-specific effects
3. There may be regional patterns

With spatial data, the goal is predict values in new regions.

For both types of data, most standard algorithms may be used.

## Dangers

Spurious correlations are a constant danger. They'll muck up any analysis. A handy tool to correlate search terms with population traits is [http://www.google.com/trends/correlate](http://www.google.com/trends/correlate).

Extrapolations can bone you too. Short term data will rarely show environmental factors that influence long term extrapolations. For example, an extrapolation of world record race times predicts that eventually, people will run the 100m in negative time.

### Example: Time Series

```{r message=FALSE}
library(quantmod)

from.dat <- as.Date(x = "01/01/08", format = "%m/%d/%y")
to.dat <- as.Date(x = "12/31/13", format = "%m/%d/%y")

# get the google stock price
getSymbols(Symbols = "GOOG", src = "yahoo", from = from.dat, to = to.dat)

head(GOOG, 3L)
```

Summarize monthly prices and store as time series
```{r}
mGoog <- to.monthly(GOOG)

googOpen <- Op(mGoog)
ts1 <- ts(googOpen, frequency=12)
plot(ts1, xlab="Years+1", ylab="GOOG")

```

## Time Series

A mathematical time series can be decomposed into 3 components:

1. Seasonal Component
2. Trend Component
3. Irregular Component

The `stats::decompose(timeSeriesObj)` function creates `decomposed.ts` object, which contains the 3 components
as additional time series, as well as the original time series data.

Decompose a time series into parts
```{r fig.height=5, fig.width=7}
plot( decompose(ts1), xlab="Years+1" )

```


### Example

Segment a time series using the `stats::window()` function
```{r}
# window indices are in years
ts1Train <- window(ts1, start=1, end=5)
ts1Test <- window(ts1, start=5, end=(7 - 0.01))
ts1Train

```


Simple Moving Average
$$Y_t = \frac{1}{2*k+1} \sum_{j=-k}^{k}{y_{t+j}}$$

Using `forecast::ma`
```{r}
library(forecast)

plot(ts1Train)
lines( ma(ts1Train, order=3), col="red")

```


## Exponential Smoothing

Exponential Smoothing gives greater weight to nearby points in time. (As long as $\alpha$ is greater than $0.5$ ??)

$$\hat{y}_{t+1} = \alpha y_t + (1 - \alpha) \hat{y}_{t-1}$$

```{r}

ets1 <- ets(ts1Train, model="MMM")
fcast <- forecast(ets1)
plot(fcast); lines(ts1Test, col="red")
```
The grayish regions are prediction boundaries.

Use the `forecast::accuracy` function to see how a model performs on a variety of metrics.
```{r}
round( accuracy(fcast, ts1Test) , 2 )
```

For forecasting, the `forecasat`, `quantmod`, and `quandl` packages are all useful. There is also a good open source book, [Forecasting: Principles and Practice](https://www.otexts.org/fpp) by Rob Hyndman.



