---
title: "c8w2_lecture_notes"
author: "R. Handsfield"
date: "September 14, 2016"
output:
  html_document:
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
---

```{r setup, include=TRUE, echo=FALSE}
knitr::opts_chunk$set(fig.align = 'center')
```
---


# The `caret` Package

The `caret` package has functions to simplify just about everything in data science/ machine learning. 

## Tutorials

* [http://edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
* [http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf](http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf)
* [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)
* [https://www.youtube.com/watch?v=7Jbb2ItbTC4](https://www.youtube.com/watch?v=7Jbb2ItbTC4)

## Installation

Install with: 
```{r, cache=TRUE, message=FALSE}
if( !("caret" %in% installed.packages()) ) {
	install.packages("caret", dependencies = c("Depends", "Suggests"), repos = "http://cran.us.r-project.org");
}
library("caret");
```

--- 

## Useful Caret Functions

__Preprocessing__

```{r, eval=FALSE}
preProcess(x, method = c("center", "scale"), thresh = 0.95,pcaComp = NULL,na.remove = TRUE,k = 5,knnSummary = mean,outcome = NULL,fudge = .2,numUnique = 3,verbose = FALSE,...)
```

__Segmenting Data__

```{r, eval=FALSE}
createDataPartition(y, times = 1, p = 0.5,list = TRUE, groups = min(5, length(y)))
```  
```{r, eval=FALSE}
createResample(y, times = 10, list = TRUE)
```  
```{r, eval=FALSE}
createFolds(y, k = 10, list = TRUE, returnTrain = FALSE)
```  
```{r, eval=FALSE}
createMultiFolds(y, k = 10, times = 5)
```  
```{r, eval=FALSE}
createTimeSlices(y, initialWindow, horizon = 1, fixedWindow = TRUE, skip = 0)
```  

__Training and Testing__

```{r, eval=FALSE}
train(x, y, method = "rf", preProcess = NULL, ..., weights = NULL, 
      metric = ifelse(is.factor(y), "Accuracy", "RMSE"), 
      maximize = ifelse(metric %in% c("RMSE", "logLoss"), FALSE, TRUE), 
      trControl = trainControl(), tuneGrid = NULL, tuneLength = 3
      )
```  
```{r, eval=FALSE}
predict (object, ...)
```  

__Comparing Models__
```{r, eval=FALSE}
confusionMatrix(data, reference, positive = NULL, 
		dnn = c("Prediction", "Reference"), 
		prevalence = NULL, mode = "sens_spec", ...
		)
```

All possible models are trained the same way -- by passing a string to `caret::train(..., method='string', ...)`. The full list of method strings is below. For details see [http://topepo.github.io/caret/using-your-own-model-in-train.html](http://topepo.github.io/caret/using-your-own-model-in-train.html)
```{r, cache=TRUE}
names(getModelInfo())
```


---

### Example: Creating Training Sets

Use `caret::createDataPartition( y=srcData, times=1, p=0.5, list=TRUE, groups=min(5, length(y)) )`:
```{r cache=TRUE, message=FALSE}
library(caret); library(kernlab); data(spam);

# creates a numeric vector that can subset from spam
inTrain<-createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]

# subsets rows NOT indexed by inTrain
testing <- spam[-inTrain, ]

dim(inTrain)

# The training set
head( spam[inTrain, 1:8 ]); 

# The test set
head(spam[-inTrain, 1:8]);
```
Note the row numbers

### Example: Fitting a Model

```{r, cache=TRUE, warning=FALSE}
set.seed(32343)

# trains a generalized linear model
modelFit <- train(type ~., data = training, method = 'glm')

# view summary of model
modelFit

# view parameters (weights/theta) of final model
modelFit$finalModel
```

### Example: Predicting with the Model

```{r, cache=TRUE}
predictions <- predict(object = modelFit, newdata = testing)

# produces a factor vector of class labels, 'spam' or 'nonspam'
predictions[1:16]
```

### Example: Evaluating a Model

```{r, cache=TRUE}
confusionMatrix(data = predictions, reference = testing$type)
```




## Caret Prediction Algorithms
* Linear Discriminant Analysis
* Regression
* Naive Bays
* SVM
* Classification & Regression Trees
* Random Forest
* Boosting
* Etc

# Data Slicing

### Example: Random Subsampling
```{r cache=TRUE, message=FALSE, eval=FALSE}
library(caret); library(kernlab); data(spam);

# creates a numeric vector that can subset from spam
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
```

```{r}
dim(training); dim(testing)
```

### Example: K-Fold Cross Val

* `k =` the number of folds
* `returnTrain=` specifies whether the folds are training or test folds
	+ values = {`TRUE`, `FALSE`}
* `list=` determines the return structure data type
	+ values = {`list`, `vector`, `matrix`}

```{r, cache=TRUE}
set.seed(32323)

# split data into 10 groups with random dropout in each
# returns a list of folds
# each fold is a numeric vector of row nums from spam$type
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=TRUE) # vs returnTrain=FALSE for test set

# display num of examples in each fold, source had 4601 rows
sapply(folds, length)

# view row dropout in various folds
folds[[1]][1:20]; folds[[2]][1:20]
```
```{r, eval=FALSE}
# use folds to index data sets via
trn2 <- spam$type[folds[[2]]]]
val2 <- spam$type[-folds[[2]]]
```
or
```{r eval=FALSE}
tst <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=FALSE) # vs returnTrain=TRUE for training set
```
Return the __training__ set via something like `df[-tst, :]`


### Example: Resampling

```{r cache=TRUE}
set.seed(32323)

# fill each fold with random sample (with replacement) of rows
folds <- createResample(y = spam$type, times = 10, list = TRUE)

# view sizes of resampled folds, note the random collection of rows
sapply(folds, length)

# note the repeated rows
folds[[1]][1:20]
```

### Example: Time Series

* `initialWindow=` rows in the _training set_
* `horizon=` the next $n$ continuous rows (_test set_)

```{r, cache=TRUE}
set.seed(32323)

tme <- 1:1000

folds <- createTimeSlices(y = tme, initialWindow = 20, horizon = 10)

# what are the factors in each of our time slices?
names(folds)

```


# Training Options

Recall the generalized linear model trained on the `spam` dataset:
```{r eval=FALSE}
library(caret); library(kernlab); data(spam);

inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

modelFit <- train(type ~., data = training, method = 'glm')
```

The `caret::train()` has a number of options to control how training is done. (View any default method parameters with `base::args(functionName.default)`)
```{r}
library(caret)
args(train.default)
```

* `preProcess = NULL, ...` takes a string vector of preprocessing methods
	+ { "BoxCox", "YeoJohnson", "expoTrans", "center", "scale", "range", "knnImpute", "bagImpute", "medianImpute", "pca", "ica" and "spatialSign"}
* `weights = NULL` takes a numeric vector of case weights, to manually up or down-weight observations
* `metric = ...` a string that specifies the cost function to minimize; by default is RMSE for numerics and fractional accuracy for factors
	+ regression: {"RMSE", "Rsquared"}
	+ classification: {"Accuracy", "Kappa"}
* `trControl = trainControl(...)` controls additional options 
	+ view via `?trainControl`

```{r}
args(trainControl)
```

* `method = boot` the _resampling_ method
	+ {"boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV", "none", "oob", "adaptive_cv", "adaptive_boot", "adaptive_LGOCV"}
* `number = ` the number of folds / resampling iterations (convergence limit??)
* `repeats = ` the number repetitions for k-fold cross validation
* `initialWindow = ` size of time series training set
* `horizon = ` size of time series test set
* `savePredictions = ` flag to return predictions from _each_ iteration of model
	+ {`"all"`, `"none"`, `"final"`} = {`TRUE`, `FALSE`, `"final"`}
* `allowParallel = ` flag to enable parallel processing 	
* `seeds = NA` initializes the random number generator to a determined state
	+ for parallel processing, seed each resample
	
	


# Plotting Predictors

```{r, echo=FALSE, cache=TRUE}
if( !("ggplot2" %in% installed.packages()) ) {
	install.packages("ggplot2", repos = "http://cran.us.r-project.org");
}

if( !("gridExtra" %in% installed.packages()) ) {
	install.packages("grid", repos = "http://cran.us.r-project.org");
	install.packages("gridExtra", repos = "http://cran.us.r-project.org");
}

library(ggplot2)
library(grid)
library(gridExtra)
```


Plots are your friends. Git gud at them. For this section, we use example data from [ISLR Package](), the companion to the book [Introduction to Statistical Learning with R]().

Install with: 
```{r, cache=TRUE, message=FALSE}
if( !("ISLR" %in% installed.packages()) ) {
	install.packages("ISLR", repos = "http://cran.us.r-project.org");
}

```

### Example: Wage Data
```{r}
library("ISLR"); library(ggplot2);

data(Wage)

summary(Wage)
```

Set aside a test set (and validation set) __before__ beginning exploratory analysis.
```{r, cache=TRUE}
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)

training <- Wage[inTrain,]
testing <- Wage[-inTrain,]

dim(training); dim(testing)
```

### Example: Lattice Plot a Subset of Wage Features

Using `caret::featurePlot()`
```{r}
# choose 3 features
mySet <- training[, c('age', 'education', 'jobclass')]

# plot wage as a function of features (4x4 = 16)
featurePlot(x=mySet, y=training$wage, plot='pairs')
```

### Example: Scatter Plot Wage vs Age

```{r}
ggplot(data=training) + aes(x=age, y=wage) + geom_point() 
```
Note the natural vertical segregation. How to investigate this further?

Answer: color the scatter by an additional feature:
```{r}
ggplot(data=training) + aes(x=age, y=wage, color=jobclass) + geom_point() 
```
Very few industrial jobs are in the upper group.

### Example: Plot with Feature Regressions

It is easy to do simple regressions inside a plot, for easy viewing. Oddly, it's actually very difficult to do your own regression, then add that to a plot. Proceed accordingly!
```{r}
g <- ggplot(data=training) + aes(x=age, y=wage, color=education) + geom_point() 

# add linear regression: wage = f(age)
g <- g + geom_smooth(method = "lm", formula = y~x)
print(g)
```
Regression lines have been added to plot

### Example: Plot by Quantiles

The method here is to pseudo-manually cut the data with `Hmisc::cut2()`, then plot the resulting quantiles.
```{r, cache=TRUE, message=FALSE}
if( !("Hmisc" %in% installed.packages()) ) {
	install.packages("Hmisc", repos = "http://cran.us.r-project.org");
}
```

```{r}
library(Hmisc)

cutWage <- cut2(training$wage, g=3)
table(cutWage)

t1 <- table(cutWage, training$jobclass)
t1

# a proportional table
prop.table(x = t1, margin = 1)  # margin=dimension
```

Plot this table to look for trends
```{r}
library(gridExtra)
g1 <- ggplot(data=training) + aes(x=cutWage, y=age, fill=cutWage) + geom_boxplot()

# view points on top of boxplot
g2 <- ggplot(data=training) + aes(x=cutWage, y=age, fill=cutWage) + geom_boxplot() + geom_jitter()
grid.arrange(g1,g2, ncol=2)
```

### Example: A Density Plot of Wage by Quantile

For continuous predictors
```{r}
ggplot(data=training) + aes(x=cutWage, color=education) + geom_density()
```



All of the above are ways to examine properties of your data set, but __don't__ use your _test_ set for exploration.

You're always looking for 

* trends
* skewed variables
* imbalance in outcomes/predictors
* outcomes not explained by predictors




# Basic Preprocessing

After plotting the variables up front, it's time to prep the data set for training.

Consider the `kernlab::spam` data set one more time:
```{r, cache=TRUE}
library(caret); library(kernlab); data(spam);

# creates a numeric vector that can subset from spam
inTrain<-createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

hist(training$capitalAve, main="", xlab="ave. capital run length")
```
This is the distribution of capital letter sequence length between spam/nonspam. This variable is highly skewed, and will bone most learning algorithms. We can redress this by shifting/stretching/squashing the input data and writing the result to an _R::caret Preprocess Object_. That object becomes the training inputs to a hypothesis function, and we pass it to the `caret::train(type= ,data=preObj, method="string")` and `caret::predict(object=preObj, newdata=groundTruth`) functions.

1. Preprocess an input set with `preObj <- preProcess(object = inputSet , method = "" )`
2. Pass the Preprocess Object to another caret function: `predict(object = preObj, newdata = groundTruth)`

## The Statistical Z-Transform

One way to fix model-boning is by doing a statistical z-transform on the data:
```{r}
testCapAve <- testing$capitalAve

# z-transform based on training set distribution
testCapAveS <- (testCapAve - mean(training$capitalAve)) / sd(training$capitalAve)

hist(testCapAveS, xlab="ave capital run length deviation")
```

Another way to do this is with the `caret::preProcess()` function:
```{r}
# col 58 is the ground-truth outcome
preObj <- preProcess( training[,-58], method=c("center", "scale") )

# pass the Preprocess object into the predict() method
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
hist(testCapAveS, xlab="ave capital run length deviation")
```

To z-transform the corresponding test set, you would create a Preprocess Object from the _training set_, then pass that object to `predict()`, along with the _testing set_:
```{r eval=FALSE}
preObj <- preProcess( training[,-58], method=c("center", "scale") )

# must pass subset of same dims to preProcess() and predict(): ex train[-58] vs test[-58]
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
```

You could also pass the `preProcess()` function as an argument to `train()`:
```{r, cache=TRUE, warning=FALSE, message=FALSE}
set.seed(32343)

modelFit <- train(type ~., data=training, method="glm", preProcess=c("center", "scale"))
modelFit
```

## Box-Cox Transform

Other preprocessing transforms are avaialable. The _Box-Cox_ is a power transform takes any continuous variable and forces it to fit a normal distribution by introducing a new parameter $\lambda$

$$
\mbox{for } y \geq 0: ~~
y(\lambda) = \left\{\begin{matrix}
\frac{ y^{\lambda}-1 }{\lambda}, & \mbox{if } \lambda \neq 0\\ 
 log(y), & \mbox{if } \lambda = 0
\end{matrix}\right.
$$

```{r cache=TRUE}
preObj <- preProcess(training[,-58], method=c("BoxCox"))

trainCapAveS <- predict(object = preObj, newdata = training[,-58])$capitalAve

par(mfrow=c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)
```

## Imputing Data

_Imputing_ is the process of assigning values to `NA`s in the data set. Imputing may use any number of interpolation methods. A common method is _k nearest neighbors_, which averages the values surrounding each `NA`. 

This is super easy, just add `"knnImpute"` to `preProcess(..., method="...")`:

```{r, cache=TRUE}
set.seed(13343)

# introduce NAs
training$capAve <- training$capitalAve  # copy a column
selectNA <- rbinom(dim(training)[1], size=1, prob=.5) == 1  # make a boolean vector
training$capAve[selectNA] <- NA  # overwrite some values with NA

# Impute and standardize
preObj <- preProcess(training[,-58], method=c("knnImpute"))
capAve <- predict(preObj, training[,-58])$capAve

# Standardize the values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth)) / sd(capAveTruth)

quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```

Be mindful that if you preprocess training data, you __cannot__ preprocess test data when predicting; you must reuse the preprocessed training data. The reason is that __any__ transformation creates new features, and creating new features on a test set will skew the predictions in unpredictable ways.


# Creating Covariates

A _covariate_ is yet another name for a model feature. In the real world, not all data variables make it into the model. Covariates (or features, or predictors) have the implicit meaning that they may have been subset from a larger group, and are intended for model inclusion.

Covariates are created in 2 levels, which sort of correspond to in-model and out-model. The goal is to include the most data in the fewest number of features (e.g. maximize data density).

* Level 1: Raw data to covariats
	+ email ascii text into numeric descriptive variables num caps, frac ngrams, etc.
	+ image into pixel subsets representing edges, corners, blobs, etc.
	+ rendering of a website into a DOM object with element parameters - color, size, etc
* Level 2: Transformivariables into tidy covariates
	+ Ex: If $A \times B$ is an important feature, create new variable $~~ \mbox{aTimesB} = A \times B$
	+ many `caret` functions to simplify this
	
## Discussion

Creating covariates is delicate balancing act between information density (summarization) and information loss. It requires substantial application knowledge and domain expertise. Including more features tends toward overfitting (high bias), which can be corrected by regularization.

* Level 1
	+ When in doubt, err on the side of more features
	+ Can be automated, but don't do it; it rarely works
* Level 2
	+ only done on training sets
	+ often necessary for regressions and SVMs
	+ rarely needed for classification trees
	+ achived through exploratory analysis
	+ new covariates should be added to data frames 
	+ use practical variable names
	
## Dummy Variables

_Dummy Variables_ are a stupid way to describe the process of converting factor levels into feature variables. If there is a factor variable with 2 levels, create a new feature for each level. Each of those features will have a boolean value for each example. This process expands the data space, widens the data set, and is also known as _one-hot encoding_.

```{r cache=TRUE}
library(ISLR); library(caret); data(Wage);

inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```

Explore the data
```{r}
table(training$jobclass)
```

Create 2 new variables for these classes with `caret::dummyVars(formula = , data = , ...)`
```{r cache=TRUE}
dummies <- dummyVars(formula = wage~jobclass, data = training)

str(dummies)

head(predict(object = dummies, newdata = training))
```
The training set now has a boolean flag for each person and job class.

## Maximizing Feature Variability

Some features, both qualitative, quantitative, and boolean, have very little variability, and won't add much information to a model. Finding numeric features of this kind is easy with `caret::nearZeroVar(x= , freqCut = 95/5, uniqueCut = 10)` or `caret::nzv()`

For `nearZeroVar()`: if `saveMetrics = FALSE`, returns a vector of integers corresponding to the column positions of the problematic predictors. If `saveMetrics = TRUE`, a data frame with columns:
```{r}
nsv <- nearZeroVar(x = training, saveMetrics = TRUE)
nsv
```
* freqRatio: the ratio of frequencies for the most common value over the second most common value
* percentUnique: the percentage of unique data points out of the total number of data points
* zeroVar: a vector of logicals for whether the predictor has __only one distinct value__
* nzv: a vector of logicals for whether the predictor __is a near zero variance predictor__

## Creating Polynomial Covariates

Recall that creating polynomial features is a simple way to improve model fit. Again, this is super easy with the splines package `splines::bs` function.

### Example: Create 2 New Features

From the `training$age` variable, we want to create $\{age, age^2, age^3 \}$:
```{r}
library(splines)

# uses B-splines with 3 dof
bsBasis <- bs(training$age, df=3)
head(bsBasis, 3L)
```

### Example: Fitting Splines

For computational efficiency, in practice the modeling code should be separated from the plotting code.
```{r cache=TRUE}
lm1 <- lm(wage~bsBasis, data=training)

# the spline will be the y-arg of a geom_line()
splineLine <- predict(object = lm1, newdata = training)

g <- ggplot(data=training) + aes(x=age, y=wage) + geom_point() 

g <- g + geom_line(aes(x=age, y=splineLine), color='red', lwd=2)
print(g)
```

* `caret:gam()` also works, but is more complicated

# Preprocessing with Principal Components (PCA)



Principal Components Analysis will segment out variables that are highly correlated.

Once again, the `kernlab::spam` dataset:
```{r, cache=TRUE}
library(caret); library(kernlab); data(spam);

# creates a numeric vector that can subset from spam
inTrain<-createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

M <- abs(cor(x = training[,-58]))
diag(M) <- 0  # remove autocorrelations
which(M > 0.9, arr.ind=TRUE) # which vars are more than 90% correlated?
```

Two features are highly correlated: `r names(spam[c(34,32)])`. Double check to be sure:
```{r}
# index by col num if you don't have the names
plot(spam[,34], spam[,32])
```
A straight line with $slope \approx 1$; this is almost perfect correlation. A weighted combination of these features might capture most of the variance while removing all the correlation. Extending this idea, we want to find the smallest matrix $M$ (subset) that captures the most variability (best explains) the original set. This matrix can be found via SVD/PCA

## The Singular Value Decomposition (SVD)

Takes an original matrix $X$ and decomposes it into a product of 3 matrices such that

$$X = UDV^T$$

* columns of $U$ are orthogonal (left singular vectors)
* $D$ is a diagonal matrix (singular values)
* columns of $V$ are orthogonal (right singular vectors)
	+ (in most real data sets, if $X$ is factorable, then $U = V$)

## The Principal Components

The Principal Components of $X$ capture the most variability of $X$ in descending cardinality order. Conveniently, __if__ the input variables have been scaled and mean-shifted (z-transformed), the principal components are equal to the columns of $V$ (often also $U$).

Principal Components are very easy to find with `stats::prcomp()`

### Example: PCA of Correlated Variables

From the previous example, columns 32 and 32 of `spam` are correlated:
```{r}
ggplot() + aes(spam[,34], spam[,32]) + geom_point()
```

If we find and plot the principal components of these variables:
```{r}
prComp <- prcomp(x = spam[,c(34,32)] )

# plot the first 2 principal components
ggplot() + aes(prComp$x[,1], prComp$x[,2]) + geom_point()
```
Mostly flat line, there is almost no correlation between the first 2 principle components of `num415` and `num857`.

What does the rotation matrix look like?
```{r}
round( prComp$rotation, digits=4)
```


### Exampple: PCA with R::stats

```{r}
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(x = log10(spam[ , -58]+1) )  # Gaussify to correct data skew (log10(...))

ggplot() + aes(prComp$x[,1], prComp$x[,2], color = typeColor) + geom_point() +
		xlab("PC1") + ylab("PC2")
```
The principal components are pretty distinct.


### Example: PCA with R::caret

During preprocessing (the `preProcess()` method), use `method="pca"`): 
```{r cache=TRUE, warning=FALSE, message=FALSE}
# log10(...) Gaussifies the input for better PCA results
preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp=2)

# the principal components matrix is actually created with predict()
spamPC <- predict(object = preProc, newdata = log10(spam[,-58]+1))

ggplot() + aes(spamPC[,1], spamPC[,2], color=typeColor) + geom_point() +
		xlab("PC1") + ylab("PC2")
```
Principal Components still pretty distinct.

### Example: Fitting to Principal Components

From the previous example, `spamPC` is the training set, the first 2 principal components of `kernlab::spam`.
```{r cache=TRUE, warning=FALSE, message=FALSE}
# scales & shifts for pca
preProc <- preProcess(log10(training[,-58]+1), method="pca", pcaComp=2)

# predict() actually does the PCA, returning the PCM
trainPC <- predict(object = preProc, newdata = log10(training[,-58]+1))

trainPC$type <- training$type
modelFit <- train(type ~., data=trainPC, method="glm")
# modelFit <- train(type ~., data = training, method = 'glm')
modelFit$results

# make some predictions with this model
testPC <- predict(preProc, log10(testing[,-58]+1))
confusionMatrix(testing$type, predict(object=modelFit, newdata=testPC))
```

A simpler form for doing the same thing is
```{r cache=TRUE, warning=FALSE, message=FALSE}
modelFit <- train(type ~ ., method="glm", preProcess="pca", data=training)
confusionMatrix(testing$type, predict(modelFit, testing))
```

In conclusion, transform data & remove outliers before doing this


# Predicting with Univariate Regression

Linear regressions are simple and easy, but limited to linear settings. With one variable (univariate), just create a line of form $y = mx + b$. In most machine learning settings, the parameters $m$ and $b$ are called _weights_, and notated with a single symbol: $\{w_0, w_1 \}$, $\{\theta_0, \theta_1\}$, etc.

In R, univariate regression is done with `caret::train()` or `stats::predict()`

### Example: Modeling Geyser Eruptions

In this example, 1 output (eruption duration) is modeled by 1 input (time between eruptions).
```{r cache=TRUE}
# data in caret::faithful
library(caret); data(faithful); library(ggplot2);

set.seed(333)

inTrain <- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]

head(trainFaith)

ggplot(data=trainFaith) + aes(x=waiting, y=eruptions) + geom_point(color='blue') +
			geom_abline(slope = 0.07, intercept = -1.5, linetype="dashed") +
			xlab("Waiting") + ylab("Duration")
```
The relationship looks pretty linear, $Duration \approx m \times Waiting + b$

---

We can find the best fit line more precisely with `stats::lm`:
```{r cache=TRUE}
library(stats)

lm1 <- lm(eruptions ~ waiting, data=trainFaith)
summary(lm1)
```

The slope and intercepts are in `Coefficients: `; slope = `waiting  0.073901`, intercept = `(Intercept)  -1.792739`.

An easy way to access the parameters is:
```{r, results='hold'}
# returns named vectors
lm1$coefficients   
# or
coef(lm1)
```

Making a single prediction is simply:
```{r}
# new imput = 80
coef(lm1)[1] + coef(lm1)[2]*80
```
Ignore the `(Intercept)` label. It's a bug.

Single predictions are even easier with `stats::predict`:
```{r}
# takes a linear model object, and data frame of new inputs
predict(object=lm1, data.frame(waiting=80) )
```


You can access all the _predictions_ of this model with: 
```{r}
lm1$fitted[1:5]  # returns a numeric vector
```

This makes plotting easy:
```{r}
ggplot(data=trainFaith) + aes(x=waiting, y=eruptions) + geom_point(color='blue') +
			geom_line(aes(x=waiting, y=lm1$fitted), color="red", lwd=1.5) +
			xlab("Waiting") + ylab("Duration")
```

### Exaple: Plot Predictions

We use `stats::predict` and the existing linear model, built on the training data. When predicting from the test set, we pass the test set into `predict()` as an argument.
```{r}
library(gridExtra)
# training set
g1 <- ggplot(data=trainFaith) + aes(x=waiting, y=eruptions) + geom_point(color='blue') +
			geom_line( aes( x=waiting, y=predict(lm1) ) ) +
			xlab("Waiting") + ylab("Duration") + ggtitle("Training Set")
print(g1)

# test set 
g2 <- ggplot(data=testFaith) + aes(x=waiting, y=eruptions) + geom_point(color='blue') +
			geom_line( aes( x=waiting, y=predict(lm1, newdata = testFaith ) ) ) +
			xlab("Waiting") + ylab("Duration") + ggtitle("Test Set")

grid.arrange(g1, g2, ncol=2)
```
Note the differing arguments in `ggplot( data =  )` and `y = predict()`. The test fit is not quite as good, which makes sense.

### Example: Training and Test Set Errors

#### MSE
```{r}
# training
sum((lm1$fitted - trainFaith$eruptions)^2)/length(trainFaith$eruptions)

# test -  substitute predict(...) for lm1$fitted
sum((  predict(lm1, newdata=testFaith) - trainFaith$eruptions)^2)/length(testFaith$eruptions)
```

#### RMSE
```{r}
# training
sqrt(sum((lm1$fitted - trainFaith$eruptions)^2)/length(trainFaith$eruptions))

# test -  substitute predict(...) for lm1$fitted
sqrt(sum((  predict(lm1, newdata=testFaith) - trainFaith$eruptions)^2)/length(testFaith$eruptions))
```

### Example: Plotting Prediction Intervals with R::stats

`stats::predict()` can calculate intervals automatically, if you pass the `interval=" "` argument.
The resulting prediction object has 3 columns: {`"fit", "lwr", "upr"`} containing the best fit predictions, as well as the corresponding plus/minus one (??) standard deviation boundaries. Unfortunately, the columnes can't be accessed with `pred1$fit`; they must be indexed via `pred1[rows, "fit"]`.
```{r cache=TRUE}
pred1 <- predict(lm1, newdata=testFaith, interval="prediction")
ord <- order(testFaith$waiting)

ggplot(data=testFaith) + aes(x=waiting, y=eruptions) + geom_point(color="blue") +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"lwr"]), color="red" ) +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"fit"]), lwd=1.2) +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"upr"]), color="red" ) 
```

### Example: Plotting Prediction Intervals with R::caret

This method passes a `caret` linear model object to `stats::predict()`, which calculates the confidence interals. 

```{r cache=TRUE}
# get a glm with caret::train()
modFit <- train(eruptions ~ waiting, data=trainFaith, method="lm")
summary(modFit$finalModel)

# pass finalModel to stats::predict()
pred1 <- predict(modFit$finalModel, newdata=testFaith, interval="prediction")
ord <- order(testFaith$waiting)

# plot it!!!
ggplot(data=testFaith) + aes(x=waiting, y=eruptions) + geom_point(color="blue") +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"lwr"]), color="red" ) +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"fit"]), lwd=1.2) +
	geom_line( aes(x=waiting[ord], y=pred1[ord,"upr"]), color="red" ) 
```



# Predicting with Multivariate Regression












Taking another look at the `ISLR::Wage` data set reveals multiple features that seem important.
```{r}
library(ISLR); library(ggplot2); library(caret);
data(Wage)

# subset out the variable we're trying to predict
Wage <- subset(Wage, select= -c(logwage))
summary(Wage)
```

At this point, create training and test sets, just for the helluvit.
```{r}
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]

dim(training); dim(testing);
```

To begin exploration, a feature plot reveals relationships between the 3 features and the output
```{r}
# caret::featurePlot()
featurePlot(x=training[, c("age", "education", "jobclass")], y=training$wage, plot="pairs")
```
Each pair plot shows a mild trend with some outliers, suggesting that each feature may be used as a predictor. 

Let's take a closer look at age
```{r}
# this first plot shows a VERY subtle trend
g1 <- ggplot(data=training) + aes(x=age, y=wage) + geom_point()

# we can plot a second feature by using color
g2 <- ggplot(data=training) + aes(x=age, y=wage, color=jobclass) + geom_point()

grid.arrange(ncol=2, g1, g2)
```
The gray plot shows that high wage correlates with middle age, but not much else.
Adding color shows that information jobs skew higher wages, and account for most of the high wage outliers.

What about education?
```{r}
g3 <- ggplot(data=training) + aes(x=age, y=wage, color=education) + geom_point()
grid.arrange(ncol=2, g2, g3)
```
Now we see that more education generally correlates with higher wage.

### Example: Fit a Multivariate Linear Model

`caret::train(formula= )` can take multiple features as x-arguments; it will then fit a model of the form

$$
y = b_0 + b_1*age + \sum_{j=1}^{J}b_2_j(jobclass_j) + \sum_{k=1}^{K}b_3_k(education_k)
$$
```{r cache=TRUE}
modFit <- train(wage ~ age+jobclass+education, method="lm", data=training)

finMod <- modFit$finalModel
print(modFit)
```
Each iteration sampled all 2102 rows of the training set. The 3 features have been expanded to 10 predictors to account for the multiple classes of jobclass and education.

The base `plot()` function will iterate through all the pairwise plots in the multivariate model (there are 6).
```{r}
plot(finMod, which = 1, cex=0.5, col="#00000010")
```
Outliers at top-left are labeled.

Sometimes, coloring by a variable not included in the model will show relationships.
```{r}
ggplot(data=training) + aes(finMod$fitted, finMod$residuals, color=training$race) + geom_point()
```
The highest outliers are all black, hmmm. What might that suggest?

### Example: Plotting Residuals v Index

Sometimes there will be systemic bias due to the row-ordering of a data set. Plotting residuals vs row-index will show this.
```{r}
ggplot() + aes(x=1:2102, y=finMod$residuals) + geom_point()
```
That doesn't seem to be the case here, but when there is a trend, it suggests that __a variable is missing from the model__.

### Example: Plotting All Covariates


```{r cache=TRUE}
modFitAll <- train(wage ~ age+jobclass+education, method="lm", data=training)
pred <- predict(modFitAll, testing)

ggplot(data=testing) + aes(x=wage, y=pred) + geom_point()
```
This plot shows the predicted wage that corresponds to each actual wage.

